## 4. 인공신경망 분석
### (1) 인공신경망 개요
* 인간의 뇌를 모방하여 만들어진 학습 및 추론 모형
* 뇌의 구조를 수학적으로 단순화해 모델링한 것
* 인공신경망 분석에서 값이 입력되면 개별 신호의 정도에 따라 값이 가중-> 가중된 값에 편향(bias)이라는 상수를 더한 후 활성함수를 거치면 인공신경망 출력값 생성
* 머신러닝->딥러닝(CNN, RNN)
* 인공신경망의 장점
  * 잡음에 민감하게 반응하지 않음
  * 비선형적 문제를 분석하는 데 유용함
  * 패턴인식, 분류, 예측 등의 문제에 효과적
  * 스스로 가중치 학습-> 다양하고 많은 데이터에 효과적
* 단점
  * 모형이 복잡할 경우 학습에 오랜 시간 걸림
  * 초기 가중치에 따라 전역해가 아닌 지역해로 수렴할 수 있음
  * 추정한 가중치의 신뢰도가 낮음
  * 결과에 대한 해석이 쉽지 않음
  * 은닉층의 수와 은닉 노드의 수를 결정하기 어려움
 
### (2) 인공신경망 알고리즘
#### 1. 활성함수
* 활성함수: 노드에 입력되는 값을 바로 다음 노드로 전달하지 않고 비선형 함수에 통과시킨 후 전달하는 함수
* 어떤 활성함수를 사용하느냐에 따라 그 출력값이 달라지므로 적절한 활성함수 사용하는 것이 중요
![SmartSelect_20240130_165417_Samsung Notes](https://github.com/qlkdkd/2-winter/assets/71871927/44dc42e2-5736-47aa-9b1d-d3f20b217642)

#### 2. 인공신경망의 계층 구조
* 하나의 인공신경망은 데이터를 입력하는 입력층, 데이터를 출력하는 출력층을 갖고 있는 단층신경망
* 입력층과 출력층 사이에 보이지 않는 다수의 은닉층을 가지고 있을 수 있는 다층신경망
* 단층신경망은 한계점 존재-> 일반적으로 인공신경망은 다층신경망 의미

* 입력층은 데이터를 입력받아 ㅅ시스템으로 전송하는 역할
* 은닉층은 입력층으로부터 값을 전달받아 가중치를 계산한 후 활성함수에 적용하여 결과를 산출하고 이를 출력층으로 보냄
* 출력층은 학습된 데이터가 포함된 층으로, 활성함수의 결과를 담고 있는 노드로 구성됨
* 출력층의 노드 수는 출력 범주의 수로 결정
* 분류 문제일 경우 출력층의 노드는 각 라벨의 확률을 포함
![SmartSelect_20240130_165802_Samsung Notes](https://github.com/qlkdkd/2-winter/assets/71871927/587b7987-5e14-43ff-bf9a-fe307d61f262)

#### 3. 인공신경망 학습(역전파 알고리즘)
* 인공신경망은 여러 개의 퍼셉트론으로 구성-> 각 퍼셉트론이 보유한 여러 개의 가중치 $w_i$값이 중요
* 인공신경망은 지도학습의 한 종류로 입력층(독립변수)와 출력층(반응변수)의 데이터에 따른 이상적인 가중치 $w_i$값을 결정해야 함

* 가중치의 값의 결정은 입력층에서 출력층으로 찾아 나가는 순전파 알고리즘을 먼저 활용
* 이때 발생한 오차들을 줄이고자 출력층에서 입력층 방향으로 거꾸로 찾아 나가는 역전파 알고리즘을 활용하여 가중치 값들을 새롭게 조정함
* 훈련용(train) 데이터의 자료들이 순차적으로 입력될때마다 가중치가 새롭게 조정되는 것을 인공신경망이 학습한다고 표현함
* 이때 전체 자료들에 의하여 학습이 한 번 되는 것을 1 epoch라 하면 일정 수의 epoch에 도달하거나 혹은 원하는 수준의 정확도를 얻을 때까지 위 작업을 반복함
![SmartSelect_20240130_170353_Samsung Notes](https://github.com/qlkdkd/2-winter/assets/71871927/b90a9e42-2c10-48be-b862-fccc48690217)

### (3) 인공신경망의 종류
#### 1. 단층 퍼셉트론(단층신경망)
* 입력층이 은닉층을 거치지 않고 바로 출력층과 연결
* 퍼셉트론은 여러 개의 개별 입력 데이터를 받아 하나의 입력 데이터로 가공하여 활성함수에 의하여 출력값 결정. 퍼셉트론의 출력값은 또 다른 퍼셉트론의 입력 데이터가 됨
* 단층 퍼셉트론은 다수의 입력값을 받아 하나의 출력값 출력:
  * 출력값이 임계값 넘음: 1 출력
  * 출력값이 임계값 못넘음: 0 출력
![SmartSelect_20240130_170853_Samsung Notes](https://github.com/qlkdkd/2-winter/assets/71871927/36b44ff4-5689-49dc-90b1-ab5b5d919531)

#### 2. 다층 퍼셉트론(다층신경망)
* 단층 퍼셉트론보다 학습하기 어려움
* 은닉층의 노드의 수가 너무 적으면 복잡한 의사결정 경계를 구축할 수 없음
* 은닉층의 노드의 수가 너무 많으면 일반화가 어렵기 때문에 과적합 문제 발생
* 너무 적은 은닉층과 은닉노드는 과소적합 문제 발생
* -> 적절한 노드의 수를 찾는 것이 중요
![SmartSelect_20240130_171053_Samsung Notes](https://github.com/qlkdkd/2-winter/assets/71871927/204c5a69-d257-4f58-b4e6-27cfbac2ba35)

### (4) 인공신경망 예

---

## 5. 나이브베이즈 분류
### (1) 베이즈 이론(Bayes Theory)
#### 1. 베이즈 이론
* 베이즈 이론은 확률을 해석하는 이론
  * 빈도확률: 객관적으로 확률 해석, 베이지안 확률: 주관적으로 확률 해석
* 빈도확률: 사건이 발생한 횟수의 장기적인 비율
  * 근본적으로 반복되는 어떤 사건의 빈도를 다루는 것
  * 모집단으로부터 반복적으로 표본을 추출했을 때 추출된 표본이 사건 A에 포함되는 경향을 사건 A의 확률이라 함
* 베이지안 확률: 사전확률과 우도확률을 통해 사후확률을 추정하는 정리
  * 데이터를 통해 확률을 추정할 때 현재 관측된 데이터의 빈도만으로 분석하는 것이 아니라 분석자의 사전지식(이미 알려진 사실 혹은 분석자의 주관)까지 포함해 분석하는 방법
* 베이즈 정리: 확률: '주장 혹은 믿음의 신뢰도', '두 확률변수의 사전 확률과 사후 확률의 사이의 관계를 나타내는 정리'
$$P(H|E)=(\frac{P(E|H)P(H)}{P(E)})$$

### (2) 나이브 베이즈 분류
#### 1. 나이브베이즈 개념
* 나이브 베이즈 분류 모델: 베이즈 정리를 기반으로 한 지도학습 모델
* 데이터의 모든 특징 변수가 서로 동등하고 독립적이라는 가정하에 분류 실행

#### 2. 나이브베이즈 알고리즘
* 이진 분류 데이터가 주어졌을 때 베이즈 이론을 통해 범주 a, b가 될 확률을 구하고, 더 큰 확률값이 나오는 범주에 데이터를 할당하는 알고리
* 범주 a에 속할 확률=$P(a|E)=(\frac{P(E|a)p(a)}{P(E)})$
* 범주 b에 속할 확률=$P(b|E)=(\frac{P(E|b)p(b)}{P(E)})$
  * P(a)와 P(b)는 사전확률로, 범주 a와 b에 해당하는 레코드를 전체 레코드로 나눈 비율을 의미함
  * P(E)는 두 수식에 겹쳐 나오므로 생량하고 계산할 수 있으며, 데이터가 변수 $v_1, v_2, v_3$으로 구성되어 있다면
$$P(v_1, v_2, v_3|E)=P(a)\times P(v_1|a)\times P(v_2|a)\times P(v_3|a)$$

---

## 6. k-NN 알고리즘
### (1) k-NN알고리즘 개요
* k-최근점 이웃이라고도 불리는 분류 알고리즘의 하나
* 지도분류에도 속하지만 군집의 특성도 가지고 있음-> 준지도학습

* k-NN은 정답 라벨이 있는 데이터들 속에서 정답 라벨이 없는 데이터들을 어떻게 분류할 것인지 대한 해결방법으로 사용

### (2) k-NN 알고리즘 원
