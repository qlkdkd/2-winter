## 03. 회귀 분석
### 1. 회귀분석 개요
### (1) 회귀분석의 개념
1. 회귀분석
  * 회귀분석: 하나 이상의 독립변수($x_1, x_2, x_3,...$들이 종속변수(y)에 얼마나 영향을 미치는지 추정하는 통계기법
  * 독립변수와 종속변수의 인과관계: 독립변수가 원인-> 종속변수에 영향을 미침
    * 독립변수=원인변수, 종속변수=겨과변수
  * 독립변수 1개-> 단순선형회귀분석, 2개 이상-> 다중선형회귀분석
  * 회귀분석은 기본적으로 연속형 변수일 때 사용, 범주형 변수일 경우 이를 파생변수로 변환하여 사용(종속변수가 범주형일 경우 로지스틱 회귀 사용)
  * 변수들이 일정한 경향성을 띔-> 그 변수들이 일정한 인과관계 가짐을 추측할 수 있음
    * 산점도를 봤을 때 일정한 추세선 나타남: 경향성 가지거나 변수들 간에 인과관계 존재
  * 아래 그림을 볼때
    * 2번 그림은 선형적 추세
    * 3번 그림은 2차 곡선 추세
    * 나머지는 어떤 추세선을 나타낸다고 보기 어려움
![image](https://github.com/qlkdkd/2-winter/assets/71871927/8eeeba14-31e7-458f-b8f1-5aad51826c59)

2. 회귀분석의 종류

종류|식|모형
---|---|---
단순회귀|$y=w_0+w_1x+b$|1개의 독립변수와 반응변수가 직선(선형)관계
다중회귀|$y=w_0+w_1x_1+w_2x_2+...+w_nx_n+b$|n개의 독립변수와 반응변수가 선형 관계
다항회귀|$y=w_0+w_1x^1+w_2x^2+...w_nx^n+b$|n개의 독립변수와 반응변수가 2차함수 이상의 관계
비선형회귀|$y=ae^{w_1x}+b$, $y=aln(w_1x)+b$, $y=asin(w_1x)+b$|회귀식이 미지의 모수들과 선형관계가 아닌 경우(지수함수, 로그함수, 삼각함수 등)

### (2) 회귀분석의 가정
* 선형성
  * 독립변수와 종속변수가 선형적
  * 2차함수 회귀선을 갖는 다항회귀분석은 선형성 필요 x
  * 산점도 통해 분석하기 전 변수 사이의 관계를 짐작할 수 있어 회귀분석 하기 전 상관분석 거의 필수적으로 함께 따라옴
 
* 독립성
  * 단순회귀분석: 잔차와 독립변수 서로 독립
  * 다중회귀분석(독립변수 여러개): 독립변수들 간에 상관성 없이 독립
  * 만약 독립변수들 상관관계 존재하는 경우: 다중공선성. 이를 제거하고 회귀분석을 수행해야 함
 
* 등분산성
  * 분산이 같다는 의미, 잔차가 고르게 분포하고 있음
  * 잔차의 중심에서 분산이 같아야 한다는 의미. 등분산성을 만족하지 못하면 회귀선은 어떤 추세를 띠지 못하고 덩어리(뭉친) 모양을 하게 됨
 
* 정규성
  * 잔차항이 정규분포 형태를 띠는 것
  * Q-Q plot에서 잔차가 오른쪽으로 상승하는 형태를 띄면 정규성 만족
 
* 오차와 잔차
  * 오차: 모집단의 데이터를 활용하여 회귀 식을 구한 경우 예측 값과 실제 값의 차이, 모집단
  * 잔차: 모집단을 특정할 수 없는 경우 모집단의 일부인 표본집단으로 회귀식 추정-> 이때 표본집단에 의해 추정된 회귀식의 예측값과 실제 값의 차이
 
* 잔차도
  * 예측값과 실제값의 차이을 나타낸 산점도
  * 예측값-실제값=0-> 잔차도=0
  * 잔차도를 활용하여 회귀식의 선형성, 등분산성의 가정성의 위배 여부 및 그에 대한 해결책을 찾을 수 있음
![image](https://github.com/qlkdkd/2-winter/assets/71871927/120cf4dc-819d-4487-943f-a4eba27a75b7)

---

### 2. 단순선형회귀분석
### (1) 회귀계수의 추정
1. 단순선형회귀분석
  * 독립변수와 종속변수가 1개씩일 때 둘 사이의 인과관계를 분석하는 것, 두 변수의 관계가 선형
  * 최소제곱법을 활용하여 실제 데이터와 오차가 가장 작아지는 직선의 방정식 찾음
$$y=w_0+w_1x+b$$
![image](https://github.com/qlkdkd/2-winter/assets/71871927/cbd5b7a9-8042-4be5-959c-001341675260)

2. 최소제곱법으로 회귀계수 추정
  * 회귀분석의 기본 알고리즘: 최소제곱법을 통해 매개변수 추정-> 추정된 매개변수를 통해 추세선을 그려 값을 예측하는 것
  * 최소제곱법: 실제 관측치와 추세선에 의해 예측된 점 사이의 거리(오차를 제곱해 더한 값)을 최소화하는 것
  * 좌표평면상에서 다양한 추세선이 그려질 수는 있지만, 잔차의 제곱 합이 최소가 되는 추세선이 가장 합리적인 추세선-> 이를 통해 회귀분석
  * 아래 그림에서 오차(파란색)제곱합이 왼쪽보다 오른쪽 직선이 더 작기 때문에 오른쪽 직선이 더 이상적인 회귀추세선
![image](https://github.com/qlkdkd/2-winter/assets/71871927/1338e6f3-74b1-4c93-9a86-ae0c31aea486)

### (2) 회귀분선모형의 적합성
1. 회귀분석의 분산분석표
  * 회귀분석의 결과에 대한 모형 적합성을 검정하기 위해서는 분산분석표 사용
  * 독립변수 1개-> 단순회귀분석, 독립변수 2개 이상-> 다중회귀분석
![image](https://github.com/qlkdkd/2-winter/assets/71871927/dfbff344-0bcf-4e70-a936-de26ed156222)
![image](https://github.com/qlkdkd/2-winter/assets/71871927/56939a5a-13c8-4937-b4ff-391799d06edd)

2. 회귀모형의 통계적 유의성 검증
  * 회귀모형의 통계적 유의성은 F검증을 통해 확인
  * F검정은 분산의 차이를 확인할 때 사용, 분산의 차이가 크다=회귀모형에서 회귀계수가 크다
  * F값이 크다: F값이 0에서 얼마나 가까운지 확률적으로 측정한 P값은 상대적으로 작아진다.
     * P값: 회귀모형에서 0.05보다 작을 경우 유의미한 인과관계가 있다고 판단하는 중요한 기준
   
3. 회귀계수의 유의성 검증
  * 회귀계수의 유의성은 t-검정을 통해 확인
     * t-통계량: 회귀계수를 표준 오차로 나눈 값-> t-통계량이 크다는 것은 분모가 작다는 의미, 분모에 해당하는 표준오차가 작다고 볼 수 있음(회귀계수(분자)>표준오차(분모))
  * t-통계량 커지면 회귀계수 커짐-> 유의미한 인과관계 검증
  * 위 회귀모형의 통계적 유의성 검증에서 살펴 t-통계량 커지면 P값 작아짐

4. 모형의 설명력
  * 회귀모형의 설명력이 좋다.: 데이터들의 분포가 회귀선에 밀접하게 분포하고 있다.
  * 회귀분석 결과를 분산분석하고, 도출된 결정계수 $R^2$로 모형의 설명력 판단
  * 결정계수 $R^2$이 1에 가깝다: 데이터들이 회귀선에 매우 밀접하게 분포한다-> 회귀모형의 예측력이 높다-> 회귀모형이 주어진 자료를 잘 설명한다

$$R^2=\frac{Q-Q_e}{Q}$$
  * $Q$: 전체 데이터들의 편차들을 제곱하여 합한 값
  * $Q_e$: 전체 데이터들의 잔차들의 제곱하여 합한 값

$$R_2=\frac{SSR}{SST}=\frac{1-SSE}{SST}=\frac{1-SSE}{SSE+SSR}=\frac{회귀모형에 의해 설명되는 변동}{총변동}$$

### (3) 단순선형회귀분석의 예
1. 자동차 배기량과 연비 회귀분석
```r
X=c(1, 1.4, 1.6, 2.0, 2.2, 2.4, 3, 3.3, 3.6)#배기량(L)
Y=c(15, 13, 13, 12, 11, 10.5, 10, 9, 8)#연비(km/L)
result=lm(Y~X)#lm: 선형모델(Linear Model)의 약어
summary(result)
```
![image](https://github.com/qlkdkd/2-winter/assets/71871927/620f2968-d7d4-4edd-b3ad-4e052d0de0ec)
  * p-value=1.97e-06>0.05: 귀무가설 기각
  * 모든 회귀계수는 0이다
  * 모형이 유의하면 각 회귀계수에 대한 p-value를 확인해야 한다.
  * X의 회귀계수: -2.4371으로 추정 가능
  * 상수항의 추정치: 16.8291
  * -> 추정되는 회귀식: $Y(연비)=\{-2.4371*(배기량 X)\}+16.8291$

2. 분산분석표와 수정계수
```r
X=c(1, 1.4, 1.6, 2.0, 2.2, 2.4, 3, 3.3, 3.6)#배기량(L)
Y=c(15, 13, 13, 12, 11, 10.5, 10, 9, 8)#연비(km/L)
result=lm(Y~X)#lm: 선형모델(Linear Model)의 약어
anova(result)
```
![image](https://github.com/qlkdkd/2-winter/assets/71871927/7508df51-1381-4811-b2da-7d42f9c9710a)
$$R^2=\frac{SSR}{SST}=\frac{SSR}{SSR+SSE}=\frac{37.25}{27.27+1.281}=0.9668$$

---

### 3. 다중선형회귀분석
### (1) 다중선형회귀분석
* 독립변수가 2개 이상이고 종속변수가 하나일때 사용 가능한 회귀분석
* 독립변수와 종속변수의 관계가 선형으로 표시
* 단순회귀분석이 확장된 형태: 기본적인 회귀계수 및 통계적 유의성 검증 등은 단순회귀분석과 같음
* 회귀계수도 여러개
$$y=w_0+w_1x_1+w_2x_2+...+w_nx_n+b$$

### (2) 다중공선성
1. 다중공선성의 개념
  * 다중공선성: 회귀분석에서 독립변수 간에 강한 상관관계가 나타나는 문제
  * 다중공선성 존재: 독립성(독립변수 간에는 상관관계 없이 독립)에 위배
  * A, B라는 변수가 있을 때 이 둘 사이에 다중공선성이 존재하면 A라는 변수가 Y값에 어느 정도의 영향으 미치는지, 또 B라는 변수가 Y값에 어느 정도의 영향을 미치는지 정확하게 판단할 수 없음
  * 다중공선성을 해결하지 않고 분서을 하면 분석 결과의 회귀계수를 신뢰할 수 없고 잘못된 결과가 나올 수 있음

2. 다중공선성의 진단
  * 결정계수 $R^2$값이 커서 회귀식의 설명력은높지만 각 독립변수 P-value값이 커서 개별 인자가 유의하지 않은 경우 다중공선성 의심할 수 있음
  * 독립변수 간의 상관계수 구함
  * 분산팽창요인(VIF)를 구해 이 값이 10이 넘는다면 보통 다중공선성이 있다고 판단 가능
$$VIF=\frac{1}{1-R^2}(R^2는 결정계수)$$

3. 다양한 다중공선성의 문제 해결법
  * 다중공선성의 문제가 발생하는 변수를 제거
  * 주성분분석(PCA)를 통해 변수의 차원을 축소
     * 차원 축소: 단순히 변수를 삭제하여 차원을 줄이는 게 아니라 원래 데이터가 가진 내재적 속성을 보존하며너 데이터를 축소하는 방법
     * R에서 스크리 산저도(Scree plot)를 사용해 주성분 개수 선택
     * 선형판별분석(LDA)으로 차원 추소
       * LDA: 지도학습으로 데이터의 분포를 학습하여 결정경계(Decision Boundary)를 만드어 데이터를 분류
     * t-분포 확률 임베딩(t-SNE)으로 차원 축소
     * 특잇값 분해(SVD)로 차원 축소. PCA와 유사한 행렬 분해 기법 사용하지만, PCA와 달리 행과 열의 크기를 다른 어떤 행렬에도 적용할 수 있다는 이점 있음
   
### (3) 다중선형회귀분석의 예
```r
yard=c(31, 31, 27, 39, 30, 32, 28, 23, 28, 35)#마당면적(m^2)
area=c(58, 51, 7, 35, 48, 42, 43, 56, 41, 41)#집 면적(m^2)
park=c(1, 1, 5, 5, 2, 4, 5, 1, 1, 3)#주차 대수(대)
dist=c(492, 426, 400, 125, 443, 412, 201, 362, 192, 423)#편의점까지의 거리(m)
price=c(12631, 12084, 12220, 15649, 11486, 12276, 15527, 12666, 13180, 10169)#집 가격(만원)

result=lm(price~yard+area+park+dist)
summary(result)
```
![image](https://github.com/qlkdkd/2-winter/assets/71871927/63cc099b-2596-40f4-abfd-1434d3667660)
* p-value=0.006267>0.05-> 귀무가설 기각
* 위 추정식은 통계적으로 유의함
* 각 독립변수에 대한 p-value값을 유의수준 0.05 이내에서 비교해보면, yard를 제외한 나머지 변수는 통계적으로 유의함
* 이 경우 yard를 제외한 나머지 3개를 독립변수로 회귀분석을 재수행할 것을 권장
* 유의미하지 않은 변수 yard를 제외하지 않는다면 추정되는 회귀식은 다음과 같음
```r
price=3045.689+(117.922*yard)+(230.563*area)+(436+801*park)+(-16.446*dist)
```
* 유의미하지 않은 변수 yard를 제외한다면 회귀분석 수행 후 위 회귀식에서 yard를 제외한 회귀식을 도출하면 됨

---

### 4. 최적 회귀방정식
### (1) 최적 회귀방정식
#### 1. 최적회귀방정식의 개념
* 1개의 반응변수 y를 설명하기 위한 k개의 독립변수 후보들이 있을 때, 반응변수 y를 가장 잘 설명할 수 있는 회귀식을 찾는 것이 최적 회귀방정식의 목표
  * 종속변수에 유의미한 영향을 미칠 것으로 생각되는 독립변수를 선택하는 과정. 보통 모델의 성능을 향상시키기 위해 사용
* 정보는 많으면 많을수록 좋으나 모든 변수를 포함하여 분석하는 것이 반드시 좋은 결과를 보장할 수 없음
  * 변수의 수가 많을 경우 일부 변수는 종속변수와 전혀 관련이 없을 수도 있고, 어떤 변수는 중복된 정보를 포함하고 있을 수 있음
  * 이러한 변수의 특성을 고려해 선택하는 것은 데이터 모델링에서 중요한 과정
* 결정계수($R^2$) 혹은 수정된 결정계수(adjuste $R^2$)도 사실 변수 선택에 활용될 수 있음

#### 2. 최적의 회귀방정식을 도출하기 위한 방법
* 변수선택법은 크게 부분집합법과 단계적 변수선택법으로 나눌 수 있음
* 부분집합법
  * 모든 가능한 모델을 고려하여 가장 좋은 모델을 선정하는 방법
  * 단점: 변수가 많아짐에 따라 검증해야 하는 회귀 분석도 많아짐
  * 변수의 개수가 적은 경우 높은 설명력을 가진 결과를 도출해 내는 데 효과적
  * '임베디드 기법'이라고도 하며 라쏘, 릿지, 엘라스틱넷 등의 다양한 방법 사용
 * 단계적 변수선택법
  * 일정한 단계를 거치면서 변수를 추가하거나 혹은 제거하는 방식으로 최적의 회귀방정식을 도출하는 방식
  * 전진선택법, 후진제거법, 단계선택법
  * 일반적으로 많이 사용됨

### (2) 변수 선택에 사용되는 성능지표
#### 1. 벌점화(패널티) 방식의 AIC와 BIC
* 회귀 모형의 변수의 수가 증가할수록 편향(bias)는 작아지고 분산(variance)은 커지려는 경향이 있음
* 그래서 변수의 수가 많아 복잡해진 모형에 벌점, 즉 일종의 패널티를 주어 최적 회귀방정식을 도출(회귀 모형의 설명력을 높이고자)하는 방법
* 결과적으로 벌점이 적은 회귀모형이 좋은(설명력이 높은=최적화된 회귀방정식) 회귀모형이라고 할 수 있음
* AIC와 BIC의 두 벌점 모두 편향과 분산이 최적이 되는 균형점 제안

#### 2. AIC(Akaike Information Criteria: 아카이케 정보 기준)
* 모델의 성능지표로서 MSE에 변수 수만큼 패널티를 주는 지표
* 일반적을 회귀분석에 Model Selection할 때 많이 쓰이는 지표
$$AIC=2LogL(\theta)+2k$$($L(\theta)$는 가능도함수, $\theta$는 최대 가능도 추정량, k는 모형의 모수 개수)

#### 3. BIC(Bayes Iformation Criteria: 베이즈 정보 기준)
* AIC의 단점인, 표본(n)이 커질 때 부정확하다는 단점을 보완한 지표
* BIC는 AIC와 큰 차이는 없지만 표본이 커질 경우 좀 더 정확한 결과가 나타난다
* BIC의 경우 변수의 개수가 많을수록 AIC보다 더 큰 패널티를 주기 때문에 변수의 개수가 적은 모형이 우선이라면 BIC를 참고하는 것이 권장된다.
$$BIC=-2LogL(\theta)+kLog(n)$$

#### 4. 멜로우 CP
* 멜로우가 제안한 통계량으로 CP갑슨 최소자승법으로 사용하여 추정된 회귀모형의 적합성을 평가하는 데 사용됨
* CP값은 수정되 결정계수($R^2$)및 AIC와 밀접한 관련이 있다.
* CP값은 모든 변수가 다 포함될 경우에 p값과 같아진다. 따라서 나쁜 모델은 CP값이 p값보다 클 때이며 좋은 모델은 최소한 p값보다 작을 때다

### (2) 단계적 변수 선택법
#### 1. 전진선택법
* 모든 독립변수 가운데 기준 통계치에 가장 많은 영향을 줄 것으로 판단되는 변수부터 하나씩 추가하면서 모형 선택
  * 설명력이 가장 높은 변수(p-value가 가장 작은 변수)부터 시작해 하나씩 모형에 추가
* 변수의 개수가 많을 때 사용 가능, 안정성 부족(변숫값이 조금만 변해도 결과에 큰 영향 미침)
* 상관계수의 절댓값이 가장 큰 변수에 대해 부분 F 검정으로 유의성 검정-> 더는 유의하지 않은 경우 해당 변수부터는 더 이상 변수 추가하지 않음

#### 2. 후진제거법
* 독립변수를 모두 포함하여 가장 적은 영향을 주는 변수부터 하나씩 제거하는 방법
* 상관계수의 절댓값이 가장 작은 변수(=유의하지 않는 변수=p-value가 큰 변수)부터 하나씩 제거
* 장점: 전체 변수의 정보 이용, 단점: 변수의 개수가 너무 많은 경우 적용하기 어려움

#### 3. 단계별 방법
* 전진선택법과 후진제거법을 보완한 방법
* 전진선택법에 의해 변수를 추가하면서 추가될 때 예상되는 벌점 값과 이미 추가된 변수가 제거될 때 예상되는 벌점 값이 가장 작도록 만들어 나가는 방법

### (3) 최적 회귀방정식 실습
```r
yard=c(31, 31, 27, 39, 30, 32, 28, 23, 28, 35)#마당면적(m^2)
area=c(58, 51, 7, 35, 48, 42, 43, 56, 41, 41)#집 면적(m^2)
park=c(1, 1, 5, 5, 2, 4, 5, 1, 1, 3)#주차 대수(대)
dist=c(492, 426, 400, 125, 443, 412, 201, 362, 192, 423)#편의점까지의 거리(m)
popul=c(4412, 2061, 4407, 1933, 4029, 4180, 3444, 1683, 3020, 4459)#인구수(명)
price=c(12631, 12084, 12220, 15649, 11486, 12276, 15527, 12666, 13180, 10169)#집 가격(만원)
result=step(lm(price~1), scope=list(lower=~1, upper=~yard+area+park+dist+popul), direction='forward')
```
![image](https://github.com/qlkdkd/2-winter/assets/71871927/cac50a40-f938-4dd6-a6b0-b95c696369c0)
